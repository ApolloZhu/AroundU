# Backend

### File structure

- Data Access Object (DAO): model database entities
    1. `Event.java`
       1. Data representation of an Event
    2. `EventParticipants.java`
       1. Data representation of all participants in an Event
    3. `User.java`
       1. Data representation of a User
    4. `UserParticipates.java`
       1. Data representation of all events a User is in
- Data Transfer Object (DTO): model HTTP request and response entities
    1. `DataResponse.java`
       1. Carries data (code, message, data object) between processes 
    2. `OperationResponse.java`
        1. Carries operational data (code, message) between processes
- Data Transfer Layer (DTL): controls data access from database
    1. `EventController.java`
       1. Manages the Event database operations such as insertion, selection or deletion.
    2. `UserController.java`
       1. Manages the User database operations such as insertion, selection or deletion.
    3. `ParticipateController.java`
       1. Manages the Participate database operations such as insertion, selection or deletion.
- Others:
    1. `SparkServer.java`
        1. Manages all APIs, connection to the database and parsing token from firebase.
    2. `CloudSqlConnectionPool.java`
        1. Generates the datasource connection pool according to the authentication info given.
    3. `CloudKmsEnvelopeAead.java`
        1. Initialize an envelope AEAD primitive for data Encryption.
## Developer Instructions
### Getting Started
- Install Maven, Spark and set up [Cloud SQL Auth proxy](https://cloud.google.com/sql/docs/mysql/connect-instance-auth-proxy)
- [Setting up Spark Server](https://sparkjava.com/documentation#getting-started)
- [Connecting to Cloud SQL database](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/main/cloud-sql/mysql/servlet/README.md)
- Put the secret API strings into a env file in /backend. The application will need those secret strings to access external dependences. Contact @John to get the secret strings.
-Make sure your GCP accout has access to the arondu-403 project.

### How to spin up the SparkServer

1. Sign in to your gcloud account. Make sure you have IAM access to the database
2. Add environment variable such as database password. Contact @John if you don't know the secret strings
3. run SparkServer

### How to deploy to GCP cloud engine
Deploy backend code by mannually triggering the deployment GitHub action. We have automated the build and deploy process by reducing variations and setting up environment variables.

### How to use Maven
1. `mvn install`
Installs the packaged code to the local Maven repository.

2. `mvn complie`
Compiles the source code, converts the .java files to .class and stores the classes in target/classes folder.

3. `mvn test`
Run junit tests under test/java

4. `mvn validate`
Validates if the project structure is correct, checking if all the dependencies have been downloaded and are available
in the local repository.

5. `mvn clean`
Clean the files and directories generated by Maven during its build

### Test APIs using Postman
- Download [Postman](https://www.postman.com)
- make sure you started the SparkServer
#### Example
Using Postman `GET` with the example request url to see the expected json data
```
http://localhost:4567/event/id?eventid=1
```
